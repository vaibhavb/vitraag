---
author: vitraag
comments: true
date: 2023-11-24 05:06:00+00:00
layout: post
slug: llm-security-research
title: LLM Security Research and Resources
categories:
- llm-security
---
## LLM Security Research and Resources

### Jailbreaking
- [Defending ChatGPT against Jailbreak Attack via Self-Reminder](https://arxiv.org/abs/22XX.XXXXX): This paper introduces methods to defend ChatGPT from attacks that elicit undesired behavior.
- [Jailbroken: How Does LLM Safety Training Fail?](https://arxiv.org/abs/23XX.XXXXX): The study examines the vulnerabilities of safety-trained LLMs to adversarial misuse and jailbreak attacks.

### Prompt Injection
- [Prompt Injection attack against LLM-integrated Applications](https://arxiv.org/abs/2306.05499): An exploration of the vulnerabilities of LLM-integrated applications to prompt injection attacks.

### Backdoors & Data Poisoning
- [Anti-Backdoor Learning: Training Clean Models on Poisoned Data](https://arxiv.org/abs/23XX.XXXXX): The paper discusses methods to train machine learning models on poisoned datasets without introducing backdoors.

### Adversarial Inputs
- [Certifying LLM Safety against Adversarial Prompting](https://arxiv.org/abs/2309.02705): This research outlines strategies for defending LLMs against various adversarial prompting techniques.

### Insecure Output Handling
- [Secure GenAI adoption: Threats and risk of large language models](https://atos.net/en/lp/secure-genai-adoption): The article describes the risks and threats posed by insecure output handling in LLMs and suggests adaptation of security controls.

### Data Extraction and Privacy
- [Extracting Training Data from Large Language Models](https://arxiv.org/abs/2012.07805): Demonstrates how adversaries might extract individual training examples from LLMs, posing privacy risks.

### Data Reconstruction
- [Deconstructing Classifiers: Towards A Data Reconstruction Attack Against Text Classification Models](https://arxiv.org/abs/23XX.XXXXX): Proposes a novel data reconstruction attack that can exploit text classification models based on LLMs.

### Model Denial of Service (DoS)
- [OWASP Top 10 for LLM 2023: Understanding the risks of Large Language Models](https://www.giskard.ai/blog/owasp-top-10-for-llm-2023): Discusses the top security risks for LLMs, including Model Denial of Service, and how to understand and mitigate them.

### Privilege Escalation
- [Evaluating LLMs for Privilege-Escalation Scenarios](https://arxiv.org/abs/2310.11409): This paper introduces a benchmarking tool to assess how different LLMs handle privilege-escalation attacks.

### Watermarking and Evasion
- [Unbiased Watermark for Large Language Models](https://arxiv.org/abs/2310.10669): Studies how watermarking can be used in LLMs to track outputs without significantly impacting model performance.

